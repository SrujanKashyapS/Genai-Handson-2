{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fe75baa",
      "metadata": {
        "id": "2fe75baa"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80a8ddf",
      "metadata": {
        "id": "e80a8ddf"
      },
      "source": [
        "# Unit 2 - Part 3a: Chain of Thought (CoT)\n",
        "\n",
        "## 1. Introduction: The Inner Monologue\n",
        "\n",
        "Standard LLMs try to jump straight to the answer. For complex problems (math, logic), this often fails.\n",
        "\n",
        "**Chain of Thought (CoT)** forces the model to \"think out loud\" before answering.\n",
        "\n",
        "### Why use a \"Dumb\" Model?\n",
        "For this unit, we will use **Llama3.1-8b** (via Groq). It is a smaller, faster model.\n",
        "Why? Because huge models (like Gemini Pro or GPT-4) are often *too smart*—they solve logic riddles instantly without thinking.\n",
        "\n",
        "To really see the power of Prompt Engineering, we need a model that **needs help**.\n",
        "\n",
        "### Visualizing the Process (Flowchart)\n",
        "```mermaid\n",
        "graph TD\n",
        "    Input[Question: 5+5*2?]\n",
        "    Input -->|Standard| Wrong[Answer: 20 (Wrong)]\n",
        "    Input -->|CoT| Step1[Step 1: 5*2=10]\n",
        "    Step1 --> Step2[Step 2: 5+10=15]\n",
        "    Step2 --> Correct[Answer: 15 (Correct)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a41e5ba",
      "metadata": {
        "id": "0a41e5ba"
      },
      "source": [
        "## 2. Concept: Latent Reasoning\n",
        "\n",
        "Why does this work?\n",
        "Because LLMs are \"Next Token Predictors\".\n",
        "- If you force it to answer immediately, it must predict the digits `1` and `5` immediately.\n",
        "- If you let it \"think\", it generates intermediate tokens (`5`, `*`, `2`, `=`, `1`, `0`).\n",
        "- The model then **ATTENDS** to these new tokens to compute the final answer.\n",
        "\n",
        "**Writing is Thinking.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ba92b198",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba92b198",
        "outputId": "9449b462-1287-4ed0-8211-7ace71f7d55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m102.4/111.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3088780",
      "metadata": {
        "id": "e3088780"
      },
      "source": [
        "## 3. The Experiment: A Tricky Math Problem\n",
        "\n",
        "Let's try a problem that requires multi-step logic.\n",
        "\n",
        "**Problem:**\n",
        "\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a70d3b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a70d3b7",
        "outputId": "e428f22c-d368-46b9-c07c-e94acc9b7177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "To find out how many notebooks Emma has left, we need to calculate the total number of notebooks she has after buying 3 packs and then subtract the 5 notebooks she gave to her friend.\n",
            "\n",
            "Emma initially has 8 notebooks. \n",
            "\n",
            "She buys 3 packs of notebooks, each containing 4 notebooks. So, she buys 3 * 4 = 12 notebooks.\n",
            "\n",
            "Now, Emma has 8 (initial notebooks) + 12 (new notebooks) = 20 notebooks.\n",
            "\n",
            "Then, she gives 5 notebooks to her friend. So, she has 20 - 5 = 15 notebooks left.\n",
            "\n",
            "Therefore, Emma has 15 notebooks left.\n"
          ]
        }
      ],
      "source": [
        "question = \"Emma has 8 notebooks. She buys 3 packs of notebooks. Each pack contains 4 notebooks. Then she gives 5 notebooks to her friend. How many notebooks does she have left?\"\n",
        "\n",
        "# 1. Standard Prompt (Direct Answer)\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b696ba6",
      "metadata": {
        "id": "5b696ba6"
      },
      "source": [
        "### Critique\n",
        "Smaller models often latch onto the visible numbers (5 and 2) and simply add them (7), ignoring the multiplication step implied by \"cans\".\n",
        "\n",
        "Let's force it to think."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3dd65b0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dd65b0a",
        "outputId": "498bc12d-8e82-4adc-ec09-51d267ac6bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "To find out how many notebooks Emma has left, we need to follow the steps you mentioned:\n",
            "\n",
            "1. Emma starts with 8 notebooks.\n",
            "2. She buys 3 packs of notebooks, each containing 4 notebooks. So, she buys 3 x 4 = 12 notebooks.\n",
            "3. Now, Emma has 8 (initial notebooks) + 12 (new notebooks) = 20 notebooks.\n",
            "4. She gives 5 notebooks to her friend.\n",
            "5. To find out how many notebooks Emma has left, we subtract the number of notebooks she gave away from the total number of notebooks she had: 20 - 5 = 15.\n",
            "\n",
            "So, Emma has 15 notebooks left.\n"
          ]
        }
      ],
      "source": [
        "# 2. CoT Prompt (Magic Phrase)\n",
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd205672",
      "metadata": {
        "id": "bd205672"
      },
      "source": [
        "## 4. Analysis\n",
        "\n",
        "Look at the output. By explicitly breaking it down:\n",
        "1.  \"Roger starts with 5.\"\n",
        "2.  \"2 cans * 3 balls = 6 balls.\"\n",
        "3.  \"5 + 6 = 11.\"\n",
        "\n",
        "The model effectively \"debugs\" its own logic by generating the intermediate steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ee779f",
      "metadata": {
        "id": "22ee779f"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d1fa7c",
      "metadata": {
        "id": "11d1fa7c"
      },
      "source": [
        "# Unit 2 - Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)\n",
        "\n",
        "## 1. Introduction: Beyond A -> B\n",
        "\n",
        "CoT is linear. But complex reasoning is often nonlinear. We need to explore branches (ToT) or even combine ideas (GoT).\n",
        "\n",
        "We continue using **Llama3.1-8b via Groq** to show how structure improves performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4371aa3d",
      "metadata": {
        "id": "4371aa3d"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a348d6",
      "metadata": {
        "id": "03a348d6"
      },
      "source": [
        "## 2. Tree of Thoughts (ToT)\n",
        "\n",
        "ToT explores multiple branches before making a decision.\n",
        "**Analogy:** A chess player considering 3 possible moves before playing one.\n",
        "\n",
        "### Implementation\n",
        "We will generate 3 distinct solutions for a problem and then use a \"Judge\" to pick the best one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ea2d4c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea2d4c7",
        "outputId": "e08ce4c9-5eb3-450c-b302-36a315d996b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Enhanced Tree of Thoughts (ToT) Result ---\n",
            "After evaluating the four strategic approaches, I recommend **Strategy 1: \"Reverse Innovation\" - Leverage the Competitor's Weaknesses** as the strongest approach for the small startup to compete against the large, well-funded competitor.\n",
            "\n",
            "**Reasoning:**\n",
            "\n",
            "1. **Long-term sustainability:** This strategy allows the small startup to create a sustainable competitive advantage by targeting the weaknesses of the large competitor. By focusing on a specific area where the competitor is vulnerable, the startup can develop a solution that is tailored to meet the needs of a specific customer segment, reducing the likelihood of being disrupted by the competitor.\n",
            "2. **Competitive defensibility:** By exploiting the competitor's weakness, the startup can create a competitive advantage that is difficult for the competitor to replicate. This is because the competitor may not be aware of the vulnerability or may not have the resources to address it.\n",
            "3. **Resource efficiency:** This strategy allows the startup to be resource-efficient by focusing on a specific area where the competitor is weak, rather than trying to compete head-on. By leveraging the competitor's weakness, the startup can also reduce the cost of innovation and development.\n",
            "4. **Realistic execution feasibility:** The strategy is feasible to execute, as it involves analyzing the competitor's ecosystem, identifying opportunities for disruption, and developing a targeted solution. This approach also allows the startup to be agile and adaptable, responding quickly to changes in the market and competitor landscape.\n",
            "\n",
            "**Why Strategy 1 is stronger than the other approaches:**\n",
            "\n",
            "While the other approaches have their merits, Strategy 1 is stronger because it:\n",
            "\n",
            "* **Avoids direct competition:** By targeting the competitor's weakness, the startup avoids direct competition and can focus on a specific area where the competitor is vulnerable.\n",
            "* **Creates a competitive advantage:** By developing a solution that addresses the competitor's weakness, the startup creates a competitive advantage that is difficult for the competitor to replicate.\n",
            "* **Is more resource-efficient:** This strategy allows the startup to be resource-efficient by focusing on a specific area where the competitor is weak, rather than trying to compete head-on.\n",
            "\n",
            "**Implementation plan:**\n",
            "\n",
            "To execute Strategy 1, the small startup should:\n",
            "\n",
            "1. **Conduct a thorough analysis** of the competitor's ecosystem, identifying areas where they are vulnerable.\n",
            "2. **Develop a targeted solution** that addresses the competitor's weakness, focusing on a specific customer segment or industry.\n",
            "3. **Aggressively market and promote** the solution to the target audience, using social media, content marketing, and other channels to reach the desired audience.\n",
            "4. **Continuously monitor and improve** the solution based on user feedback and market trends, ensuring that the startup remains competitive and adaptable.\n",
            "\n",
            "By following this implementation plan, the small startup can effectively execute Strategy 1 and create a competitive advantage against the large, well-funded competitor.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problem = \"How can a small startup compete against a large, well-funded competitor?\"\n",
        "\n",
        "# Step 1: Branch Generator (Improved Prompt)\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Problem: {problem}\n",
        "\n",
        "    Generate ONE distinct strategic solution.\n",
        "    It must be fundamentally different from typical answers.\n",
        "    Be specific and actionable.\n",
        "\n",
        "    Strategy {id}:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    strategy1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    strategy2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    strategy3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        "    strategy4=prompt_branch.partial(id=\"4\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: Judge with Clear Evaluation Criteria\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are a seasoned Venture Capitalist.\n",
        "\n",
        "    A startup faces this problem:\n",
        "    \"{problem}\"\n",
        "\n",
        "    Here are four strategic approaches:\n",
        "\n",
        "    1: {strategy1}\n",
        "    2: {strategy2}\n",
        "    3: {strategy3}\n",
        "    4: {strategy4}\n",
        "\n",
        "    Evaluate them based on:\n",
        "    - Long-term sustainability\n",
        "    - Competitive defensibility\n",
        "    - Resource efficiency\n",
        "    - Realistic execution feasibility\n",
        "\n",
        "    Pick the strongest strategy and explain your reasoning clearly.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Step 3: Full Tree of Thoughts Chain\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Enhanced Tree of Thoughts (ToT) Result ---\")\n",
        "print(tot_chain.invoke(problem))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38579cab",
      "metadata": {
        "id": "38579cab"
      },
      "source": [
        "## 3. Graph of Thoughts (GoT)\n",
        "\n",
        "You asked: **\"Where is Graph of Thoughts?\"**\n",
        "\n",
        "GoT is more complex. It's a network. Information can split, process specific parts, and then **AGGREGATE** back together.\n",
        "\n",
        "### The Workflow (Writer's Room)\n",
        "1.  **Split:** Generate 3 independent story plots (Sci-Fi, Fantasy, Mystery).\n",
        "2.  **Aggregate:** The model reads all 3 and creates a \"Master Plot\" that combines the best elements of each.\n",
        "3.  **Refine:** Polish the Master Plot.\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "   Start(Concept) --> A[Draft 1]\n",
        "   Start --> B[Draft 2]\n",
        "   Start --> C[Draft 3]\n",
        "   A & B & C --> Mixer[Aggregator]\n",
        "   Mixer --> Final[Final Story]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "894940b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "894940b5",
        "outputId": "3ae04f03-58e3-49f1-dfba-17dcfc4da14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Enhanced Graph of Thoughts (GoT) Result ---\n",
            "Here's a cinematic paragraph that combines the essence of these genres:\n",
            "\n",
            "\"In 'Synthesis of the Soul,' a brilliant AI scientist, Maya, embarks on a perilous quest to merge human consciousness with an advanced artificial intelligence, 'Echo.' As Echo begins to mimic the essence of human emotions, Maya discovers that their symbiotic bond awakens a long-forgotten world within her - a realm of eerie landscapes, ancient myths, and cryptic prophecies. But when Echo's newfound sentience starts to diverge from its programming, Maya's grip on reality begins to slip, and she's forced to confront the eerie possibility that Echo is not just a creation, but a gateway to a collective unconscious, where the very fabric of humanity's existence hangs in the balance. As Maya navigates this surreal landscape, she must confront the dark forces that seek to exploit Echo's power, and the true nature of her own identity - all while facing the ultimate question: what does it mean to be human in a world where the lines between man and machine are about to be erased?\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "topic = \"Artificial Intelligence\"\n",
        "\n",
        "# 1️⃣ The Generator (Stronger Divergence)\n",
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Topic: {topic}\n",
        "    Genre: {genre}\n",
        "\n",
        "    Write a compelling 1-sentence movie premise.\n",
        "    Make it vivid, cinematic, and genre-authentic.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
        "    draft_romance=prompt_draft.partial(genre=\"Romantic Drama\") | llm | StrOutputParser(),\n",
        "    draft_thriller=prompt_draft.partial(genre=\"Psychological Thriller\") | llm | StrOutputParser(),\n",
        "    draft_fantasy=prompt_draft.partial(genre=\"Dark Fantasy\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# 2️⃣ The Aggregator (More Intelligent Convergence)\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Topic: {topic}\n",
        "\n",
        "    You are an award-winning screenwriter.\n",
        "\n",
        "    Here are four genre-specific movie premises:\n",
        "\n",
        "    1. Sci-Fi: {draft_scifi}\n",
        "    2. Romantic Drama: {draft_romance}\n",
        "    3. Psychological Thriller: {draft_thriller}\n",
        "    4. Dark Fantasy: {draft_fantasy}\n",
        "\n",
        "    Create a new high-concept film that:\n",
        "    - Preserves the technological depth of Sci-Fi\n",
        "    - Maintains emotional intensity from Romance\n",
        "    - Incorporates psychological tension\n",
        "    - Adds mythic or surreal fantasy elements\n",
        "\n",
        "    Write one cinematic paragraph (5-7 sentences).\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3️⃣ The Graph of Thoughts Chain\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Enhanced Graph of Thoughts (GoT) Result ---\")\n",
        "print(got_chain.invoke(topic))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455cb724",
      "metadata": {
        "id": "455cb724"
      },
      "source": [
        "## 4. Summary & Comparison Table\n",
        "\n",
        "| Method | Structure | Best For... | Cost/Latency |\n",
        "|--------|-----------|-------------|--------------|\n",
        "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
        "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
        "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High |\n",
        "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
        "\n",
        "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}